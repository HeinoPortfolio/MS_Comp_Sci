{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc1b3d8",
   "metadata": {},
   "source": [
    "CS 621 Foundation of Data Analytics\n",
    "Module 11\n",
    "\n",
    "Author:  Matthew Heino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887a761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e1759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open('amazon_reviews.txt', 'r')\n",
    "\n",
    "amazon_text = []\n",
    "\n",
    "for line in in_file:\n",
    "    amazon_text.append(line)\n",
    "    \n",
    "append_test = []\n",
    "for index in range(len(amazon_text)):\n",
    "    append_test.append(amazon_text[index])\n",
    "\n",
    "list_of_sentences = []\n",
    "\n",
    "# Break into sentences\n",
    "for item in append_test:\n",
    "    list_of_sentences.append(sent_tokenize(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094ca0f",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which of the following description is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c9bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: \n",
      "Number of all words is:   3143945\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = []\n",
    "\n",
    "# Toekenize the sentences.\n",
    "for index1 in range(0, len(list_of_sentences)):\n",
    "    \n",
    "    for index2 in range(0, len(list_of_sentences[index1])): \n",
    "\n",
    "        new_sentence = list_of_sentences[index1][index2].translate(str.maketrans('','',string.punctuation))\n",
    "        \n",
    "        tokenize_words.extend(word_tokenize(new_sentence))\n",
    " \n",
    "print(\"\\nQuestion 1: \\nNumber of all words is:  \",len(tokenize_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab146114",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "For each review in the provided dataset, do the following:\n",
    "\n",
    " - Remove punctuation marks\n",
    " - Make every word in lower case\n",
    " - Lemmatize each word\n",
    " - Put the processed words of each review in one single line and output all lines into a text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1470dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "final_words = []\n",
    "\n",
    "for index1 in range(0, len(list_of_sentences)):\n",
    "    \n",
    "    # Break into sentences\n",
    "    list_of_sentences = []\n",
    "    \n",
    "    lemma_words_lst = []\n",
    "  \n",
    "    list_of_sentences.extend(sent_tokenize(append_test[index1]))\n",
    "    \n",
    "    lower_case = []\n",
    "    \n",
    "    for sentence in  list_of_sentences:\n",
    "        \n",
    "        # Remove the punctuation marks\n",
    "        sentence = sentence.translate(str.maketrans('','',string.punctuation))\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        lower_case.append(sentence)\n",
    "        \n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            \n",
    "            # lemmatize the word.\n",
    "            lemma_word = lem.lemmatize(word)\n",
    "            \n",
    "            # Add to lemmatized words_list\n",
    "            lemma_words_lst.append(lemma_word)\n",
    "        \n",
    "    # add to the list\n",
    "    final_words.append(lemma_words_lst)\n",
    "        \n",
    " \n",
    "output_list = []\n",
    " \n",
    "for index in range (0, len(final_words)):\n",
    "    \n",
    "    word_string = \"\"\n",
    "    \n",
    "    for index2 in range(0, len(final_words[index])):\n",
    "        \n",
    "        word_string = word_string + ' ' + final_words[index][index2]\n",
    "    \n",
    "    # append to the main list of \n",
    "    output_list.append(word_string.lstrip())  \n",
    "    \n",
    "# write list to the file\n",
    "with open('cs621_module_11_Heino.txt', 'w+') as outfile:\n",
    "    \n",
    "    for item in output_list:\n",
    "        outfile.write('%s\\n' %item)\n",
    "\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
